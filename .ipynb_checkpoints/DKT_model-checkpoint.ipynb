{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DKT Model\n",
    "\n",
    "This file trains a DKT model with Assistment data and tests the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data we pre-processed. It contains each student's action sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'student', u'skill', u'correct'], dtype='object')\n",
      "200000 problem records\n",
      "5 skills\n",
      "4000 students\n",
      "121560 correct answers\n"
     ]
    }
   ],
   "source": [
    "# Limit the data size\n",
    "skill_cut = 150        # limit skill amounts\n",
    "student_cut = 5000    # limit sequences\n",
    "\n",
    "#dataset = pd.read_csv(\"Assistments/assistment_for_dkt.csv\")\n",
    "dataset = pd.read_csv(\"synthetic/set_10.csv\")\n",
    "\n",
    "dataset = dataset[dataset['skill'] < skill_cut]\n",
    "print dataset.columns\n",
    "num_records = len(dataset)\n",
    "num_skills = len(dataset['skill'].value_counts())\n",
    "num_actions = 2 * num_skills    # action: every skill correct/wrong\n",
    "num_labels = num_skills + 1     # one-hot question, plus one bit for correct/wrong\n",
    "num_students = len(dataset['student'].value_counts())\n",
    "print str(num_records) + \" problem records\"\n",
    "print str(num_skills) + \" skills\"\n",
    "print str(num_students) + \" students\"\n",
    "print str(np.sum(dataset['correct'].values)) + \" correct answers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following LSTM is based on the one in the Udacity Assignment. The structure of LSTM is the one introduced in this [article](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper parameters to Tune\n",
    "num_hidden = 200\n",
    "init_mean = 0\n",
    "init_stddev = 0.001\n",
    "# batch_size sequences, with the length of time_window\n",
    "batch_size = 100\n",
    "time_window = 50\n",
    "# Training\n",
    "# We are using Adams Optimizer, so no hyperparameter.\n",
    "clipping_norm = 2\n",
    "dropout_keep = 1.0\n",
    "\n",
    "train_ratio = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assistments Tuning\n",
    "|set| num hidden | init mean | init stddev | batch size |time window|clipping norm|Dropout| AUC    | Overfit After |\n",
    "|:-:|:----------:|:---------:|:-----------:|:----------:|:---------:|:-----------:|:-----:|:------:|:-------------:|\n",
    "| 1 |     200    |     0     |    0.001    |     50     |    50     |     10      |   1   | 0.8152 | epoch 8       |\n",
    "| 2 |     200    |     0     |    0.001    |    100     |    50     |     10      |   1   | 0.8172 | epoch 9       |\n",
    "| 3 |     200    |     0     |    0.001    |    100     |    50     |      5      |   1   | 0.8173 | epoch 9       |\n",
    "| 4 |     200    |     0     |    0.001    |    100     |    50     |      2      |   1   | 0.8177 | epoch 8       | \n",
    "| 5 |     200    |   0.01    |    0.001    |    100     |    50     |      2      |   1   | 0.8152 | epoch 10      |\n",
    "| 6 |     200    |     0     |    0.001    |    100     |   100     |      2      |   1   | 0.8169 | epoch 19      |\n",
    "| 7 |     200    |     0     |    0.001    |    100     |    50     |      2      |  0.5  | 0.8174 | epoch 13      | \n",
    "| 8 |     200    |     0     |    0.001    |    100     |    50     |      2      |  0.5  | 0.8250 | epoch 15      | \n",
    "| 9 |     200    |     0     |    0.001    |    100     |    50     |      2      |  0.2  | 0.8185 | epoch 20+     |\n",
    "|10 |     200    |     0     |    0.001    |    100     |    50     |      2      |   1   | 0.8248 | epoch 8       |\n",
    "\n",
    "*Set 1-7 use 60% data to train, the rest to test.*\n",
    "*Set 8- use 80% data to train, the rest to test. (As Piech did)*\n",
    "\n",
    "As for now, none of the hyperparameters seem to have a major influence on the performance. So probaly we'll just leave it here. Note that we are just using the default AdamOptimizer and haven't tuned even one bit.\n",
    "\n",
    "The padding seems matter a lot, since batch size and time window seem to affect the performance a lot. A larger time window results in slower convergence, and looks like less prone to overfit.\n",
    "\n",
    "AUC drop in one epoch does not necessarily mean that the model has overfitted. However, our model seems to overfit merely after 10 epoches, therefore we need to add regularization tricks, like dropout.\n",
    "In this table, dropout means the keep_prob. According to set 7, it does help to reduce overfitting, yet it doesn't seem to improve the performance.\n",
    "\n",
    "\n",
    "#### Synthetic Tuning\n",
    "The followings are the results for the synthetic data. We use 50% data to train, and the rest to test (as Mozer's work did)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters: _x for new input, _m for old output, _b for bias\n",
    "    # Input gate\n",
    "    input_x = tf.Variable(tf.truncated_normal([num_actions, num_hidden], init_mean, init_stddev))\n",
    "    input_m = tf.Variable(tf.truncated_normal([num_hidden, num_hidden], init_mean, init_stddev))\n",
    "    input_b = tf.Variable(tf.zeros([1, num_hidden]))\n",
    "    # Forget gate\n",
    "    forget_x = tf.Variable(tf.truncated_normal([num_actions, num_hidden], init_mean, init_stddev))\n",
    "    forget_m = tf.Variable(tf.truncated_normal([num_hidden, num_hidden], init_mean, init_stddev))\n",
    "    forget_b = tf.Variable(tf.zeros([1, num_hidden]))\n",
    "    # Update cell:                             \n",
    "    update_x = tf.Variable(tf.truncated_normal([num_actions, num_hidden], init_mean, init_stddev))\n",
    "    update_m = tf.Variable(tf.truncated_normal([num_hidden, num_hidden], init_mean, init_stddev))\n",
    "    update_b = tf.Variable(tf.zeros([1, num_hidden]))\n",
    "    # Output gate:\n",
    "    output_x = tf.Variable(tf.truncated_normal([num_actions, num_hidden], init_mean, init_stddev))\n",
    "    output_m = tf.Variable(tf.truncated_normal([num_hidden, num_hidden], init_mean, init_stddev))\n",
    "    output_b = tf.Variable(tf.zeros([1, num_hidden]))\n",
    "    # Variables saving state across the sequence (length: time_window).\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_hidden]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_hidden]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    classify_w = tf.Variable(tf.truncated_normal([num_hidden, num_skills], init_mean, init_stddev))\n",
    "    classify_b = tf.Variable(tf.zeros([num_skills]))\n",
    "  \n",
    "    def lstm_train_cell(i, o, state):\n",
    "        # input, last/saved_output, last/saved_state\n",
    "        input_gate = tf.sigmoid(tf.nn.dropout(tf.matmul(i, input_x), dropout_keep) + tf.matmul(o, input_m) + input_b)\n",
    "        forget_gate = tf.sigmoid(tf.nn.dropout(tf.matmul(i, forget_x), dropout_keep) + tf.matmul(o, forget_m) + forget_b)\n",
    "        update = tf.tanh(tf.nn.dropout(tf.matmul(i, update_x), dropout_keep) + tf.matmul(o, update_m) + update_b)\n",
    "        state = forget_gate * state + input_gate * update\n",
    "        output_gate = tf.sigmoid(tf.nn.dropout(tf.matmul(i, output_x), dropout_keep) + tf.matmul(o, output_m) + output_b)\n",
    "        # return new_output, new_state\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    inputs = list()\n",
    "    question_labels = list()\n",
    "    action_labels = list()    # only when training\n",
    "    for _ in range(time_window):\n",
    "        inputs.append(tf.placeholder(tf.float32, shape=[batch_size, num_actions]))\n",
    "        question_labels.append(tf.placeholder(tf.float32, shape=[batch_size, num_skills]))\n",
    "        action_labels.append(tf.placeholder(tf.float32, shape=[batch_size, ]))\n",
    "    \n",
    "    # State resets when starting a new sequence\n",
    "    reset_state = tf.group(saved_output.assign(tf.zeros([batch_size, num_hidden])),\n",
    "                           saved_state.assign(tf.zeros([batch_size, num_hidden])))\n",
    "    \n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in inputs:\n",
    "        output, state = lstm_train_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across different segment of a sequence\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), classify_w, classify_b)\n",
    "        # logits of the actual encountered problem:\n",
    "        logits_of_interest = tf.reduce_sum(tf.mul(logits, tf.concat(0, question_labels)), 1)\n",
    "        truth = tf.reshape(tf.concat(0, action_labels), [-1])    # flatten\n",
    "        # binary cross entropy: padding would introduce some constant loss\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits_of_interest, truth))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    gradients, var = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, clipping_norm)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, var))\n",
    "    \n",
    "    prediction = tf.sigmoid(logits_of_interest)\n",
    "    \n",
    "    # Testing\n",
    "    def lstm_test_cell(i, o, state):\n",
    "        # no dropout\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, input_x) + tf.matmul(o, input_m) + input_b)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, forget_x) + tf.matmul(o, forget_m) + forget_b)\n",
    "        update = tf.tanh(tf.matmul(i, update_x) + tf.matmul(o, update_m) + update_b)\n",
    "        state = forget_gate * state + input_gate * update\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, output_x) + tf.matmul(o, output_m) + output_b)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    test_outputs = list()\n",
    "    test_output = saved_output\n",
    "    test_state = saved_state\n",
    "    for i in inputs:\n",
    "        test_output, test_state = lstm_test_cell(i, test_output, test_state)\n",
    "        test_outputs.append(test_output)\n",
    "\n",
    "    with tf.control_dependencies([saved_output.assign(test_output), saved_state.assign(test_state)]):\n",
    "        test_logits = tf.nn.xw_plus_b(tf.concat(0, test_outputs), classify_w, classify_b)\n",
    "        test_logits_of_interest = tf.reduce_sum(tf.mul(test_logits, tf.concat(0, question_labels)), 1)\n",
    "    \n",
    "    test_status = tf.sigmoid(test_logits)\n",
    "    test_prediction = tf.sigmoid(test_logits_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genrating input sequences for LSTM is a bit complicated. The general idea is first take a batch of students then pad their sequence to the same length. When feeding to LSTM, we feed one \"window\"(time interval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataGenerator(object):\n",
    "    def __init__(self, dataset, train_ratio):\n",
    "        # convert file to sequence\n",
    "        dataset = dataset.values\n",
    "        seqs = list()\n",
    "        last_student = -1\n",
    "        print dataset.shape\n",
    "        for i in range(len(dataset)):\n",
    "            if dataset[i][0] != last_student:    # a new student\n",
    "                last_student = dataset[i][0]\n",
    "                seqs.append([(dataset[i][1], dataset[i][2])])  # (skill, correct)\n",
    "            else:     # same student\n",
    "                seqs[-1].append((dataset[i][1], dataset[i][2]))\n",
    "        del dataset\n",
    "        \n",
    "        tot_seqs = min(len(seqs), student_cut)\n",
    "        print \"total: %d sequences\" % tot_seqs\n",
    "        \n",
    "        # split train and test\n",
    "        train_size = int(tot_seqs * train_ratio)\n",
    "        train_seq_cnt = 0\n",
    "        self._train_seqs = list()\n",
    "        for i in range(train_size):\n",
    "            self._train_seqs.append(seqs[i])\n",
    "            train_seq_cnt += len(seqs[i])\n",
    "        test_seq_cnt = 0\n",
    "        self._test_seqs = list()\n",
    "        for i in range(train_size, tot_seqs):\n",
    "            self._test_seqs.append(seqs[i])\n",
    "            test_seq_cnt += len(seqs[i])\n",
    "        print \"%d records for train\" % train_seq_cnt\n",
    "        print \"%d records for test\" % test_seq_cnt\n",
    "        self._tot_train_record = train_seq_cnt\n",
    "        self._tot_test_record = test_seq_cnt\n",
    "        \n",
    "        # takes around 2GB memory:\n",
    "        self._train_inputs = []\n",
    "        self._train_labels = []\n",
    "        self.generate_batch(self._train_seqs, self._train_inputs, self._train_labels)\n",
    "        \n",
    "        self._test_inputs = []\n",
    "        self._test_labels = []\n",
    "        self.generate_batch(self._test_seqs, self._test_inputs, self._test_labels)\n",
    "        \n",
    "        print \"all batch generated\"\n",
    "        \n",
    "        self._train_cursor = -1\n",
    "        self._test_cursor = -1\n",
    "        \n",
    "    def get_train_batch_num(self):\n",
    "        return len(self._train_inputs)\n",
    "    \n",
    "    def get_test_batch_num(self):\n",
    "        return len(self._test_inputs)\n",
    "    \n",
    "    def get_train_batch(self):\n",
    "        self._train_cursor += 1\n",
    "        if self._train_cursor == len(self._train_inputs):\n",
    "            self._train_cursor = 0\n",
    "        return self._train_inputs[self._train_cursor], self._train_labels[self._train_cursor]\n",
    "    \n",
    "    def get_test_batch(self):\n",
    "        self._test_cursor += 1\n",
    "        if self._test_cursor == len(self._test_inputs):\n",
    "            self._test_cursor = 0\n",
    "        return self._test_inputs[self._test_cursor], self._test_labels[self._test_cursor]\n",
    "    \n",
    "    def generate_batch(self, seqs_pool, inputs, labels):\n",
    "        seq_count = len(seqs_pool)\n",
    "        num_batch = int(math.ceil(float(seq_count) / batch_size))\n",
    "        correct_cnt = 0\n",
    "        for start in range(0, seq_count, batch_size):            \n",
    "            end = min(seq_count, start + batch_size)\n",
    "            maxlen = 0\n",
    "            for i in range(start, end):\n",
    "                if maxlen < len(seqs_pool[i]):\n",
    "                    maxlen = len(seqs_pool[i])\n",
    "            num_window = int(math.ceil(float(maxlen) / time_window))\n",
    "            \n",
    "            # setup empty data (i.e., padded with full 0s)\n",
    "            inputs.append([])\n",
    "            labels.append([])\n",
    "            for _ in range(num_window):\n",
    "                inputs[-1].append([])\n",
    "                labels[-1].append([])\n",
    "                for _ in range(time_window):\n",
    "                    inputs[-1][-1].append(np.zeros([batch_size, num_actions], dtype=np.float32))\n",
    "                    labels[-1][-1].append(np.zeros([batch_size, num_labels], dtype=np.float32))\n",
    "            \n",
    "            # fill in data\n",
    "            for i in range(start, end):\n",
    "                pos_in_batch = i - start    # position in batch\n",
    "                seq = seqs_pool[i]\n",
    "                # from back to front\n",
    "                for back_offset in range(1, len(seq) + 1):\n",
    "                    # find the row of the record\n",
    "                    window_offset = - int(math.ceil(float(back_offset) / time_window))\n",
    "                    frame_offset = - back_offset % time_window\n",
    "                    if frame_offset == 0:\n",
    "                        frame_offset = - time_window\n",
    "                    # code the record by setting ones\n",
    "                    record = seq[- back_offset]\n",
    "                    labels[-1][window_offset][frame_offset][pos_in_batch][record[0]] = 1\n",
    "                    labels[-1][window_offset][frame_offset][pos_in_batch][num_skills] = record[1]\n",
    "                    \n",
    "                    input_back_offset = back_offset - 1    # skew input backward 1 time step\n",
    "                    if input_back_offset == 0:\n",
    "                        continue\n",
    "                    input_window_offset = - int(math.ceil(float(input_back_offset) / time_window))\n",
    "                    input_frame_offset = - input_back_offset % time_window\n",
    "                    if input_frame_offset == 0:\n",
    "                        input_frame_offset = - time_window\n",
    "                    inputs[-1][input_window_offset][input_frame_offset][pos_in_batch][2 * record[0] + record[1]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following session trains and runs the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 3)\n",
      "total: 4000 sequences\n",
      "100000 records for train\n",
      "100000 records for test\n",
      "all batch generated\n"
     ]
    }
   ],
   "source": [
    "# Running Specifications\n",
    "num_epochs = 50\n",
    "test_frequency = 1\n",
    "output_frequency = 3\n",
    "\n",
    "data_generator = DataGenerator(dataset, train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = 13.6827976704\n",
      "Train AUC = 0.515000529531\n",
      "Test AUC = 0.584912453392    \n",
      "epoch 1: loss = 13.1435196996\n",
      "Train AUC = 0.600681757112\n",
      "Test AUC = 0.612999938733    \n",
      "epoch 2: loss = 12.9146568179\n",
      "Train AUC = 0.633367989542\n",
      "Test AUC = 0.64319886649    \n",
      "epoch 3: loss = 12.7293676734\n",
      "Train AUC = 0.651073460989\n",
      "Test AUC = 0.65011028235    \n",
      "epoch 4: loss = 12.6690826416\n",
      "Train AUC = 0.652727859873\n",
      "Test AUC = 0.64929161729    \n",
      "epoch 5: loss = 12.6387900114\n",
      "Train AUC = 0.65455916542\n",
      "Test AUC = 0.651058855692    \n",
      "epoch 6: loss = 12.6218128204\n",
      "Train AUC = 0.655307708842\n",
      "Test AUC = 0.651616834941    \n",
      "epoch 7: loss = 12.6067184806\n",
      "Train AUC = 0.656217494866\n",
      "Test AUC = 0.652427379945    \n",
      "epoch 8: loss = 12.5932086706\n",
      "Train AUC = 0.656948762235\n",
      "Test AUC = 0.653249392922    \n",
      "epoch 9: loss = 12.5792400837\n",
      "Train AUC = 0.657703359308\n",
      "Test AUC = 0.65422925813    \n",
      "epoch 10: loss = 12.5626182556\n",
      "Train AUC = 0.658566439879\n",
      "Test AUC = 0.655813939798    \n",
      "epoch 11: loss = 12.5413085818\n",
      "Train AUC = 0.659784607965\n",
      "Test AUC = 0.657976432806    \n",
      "epoch 12: loss = 12.5199841261\n",
      "Train AUC = 0.66138669208\n",
      "Test AUC = 0.660239055582    \n",
      "epoch 13: loss = 12.4978470802\n",
      "Train AUC = 0.663395300695\n",
      "Test AUC = 0.662998918318    \n",
      "epoch 14: loss = 12.475823164\n",
      "Train AUC = 0.666109614217\n",
      "Test AUC = 0.668037467571    \n",
      "epoch 15: loss = 12.4315416813\n",
      "Train AUC = 0.671766509981\n",
      "Test AUC = 0.677489557633    \n",
      "epoch 16: loss = 12.335880816\n",
      "Train AUC = 0.681457398604\n",
      "Test AUC = 0.693051742457    \n",
      "epoch 17: loss = 12.1202693582\n",
      "Train AUC = 0.704170502977\n",
      "Test AUC = 0.718220444508    \n",
      "epoch 18: loss = 11.9386513829\n",
      "Train AUC = 0.71749951559\n",
      "Test AUC = 0.728160416296    \n",
      "epoch 19: loss = 11.7596942782\n",
      "Train AUC = 0.732455203441\n",
      "Test AUC = 0.739244766251    \n",
      "epoch 20: loss = 11.5707788467\n",
      "Train AUC = 0.744285803953\n",
      "Test AUC = 0.749340169831    \n",
      "epoch 21: loss = 11.3839870691\n",
      "Train AUC = 0.755880673955\n",
      "Test AUC = 0.759906781136    \n",
      "epoch 22: loss = 11.2648643851\n",
      "Train AUC = 0.762305830621\n",
      "Test AUC = 0.765429507038    \n",
      "epoch 23: loss = 11.1965470314\n",
      "Train AUC = 0.764819978517\n",
      "Test AUC = 0.767095254305    \n",
      "epoch 24: loss = 11.1482719183\n",
      "Train AUC = 0.767164801464\n",
      "Test AUC = 0.767083500436    \n",
      "epoch 25: loss = 11.1808956265\n",
      "Train AUC = 0.764735513722\n",
      "Test AUC = 0.760655863074    \n",
      "epoch 26: loss = 11.2597179413\n",
      "Train AUC = 0.760207928145\n",
      "Test AUC = 0.759158829324    \n",
      "epoch 27: loss = 11.3604115844\n",
      "Train AUC = 0.75418676644\n",
      "Test AUC = 0.753985269611    \n",
      "epoch 28: loss = 11.4929702878\n",
      "Train AUC = 0.745256927038\n",
      "Test AUC = 0.745642449148    \n",
      "epoch 29: loss = 11.466787219\n",
      "Train AUC = 0.745677471588\n",
      "Test AUC = 0.751277127639    \n",
      "epoch 30: loss = 11.2891964912\n",
      "Train AUC = 0.757998897937\n",
      "Test AUC = 0.760170083309    \n",
      "epoch 31: loss = 11.1291591525\n",
      "Train AUC = 0.767050041625\n",
      "Test AUC = 0.76780707232    \n",
      "epoch 32: loss = 11.1040780544\n",
      "Train AUC = 0.768564010838\n",
      "Test AUC = 0.768486843999    \n",
      "epoch 33: loss = 11.1629878879\n",
      "Train AUC = 0.765001307795\n",
      "Test AUC = 0.769175722492    \n",
      "epoch 34: loss = 11.3598406315\n",
      "Train AUC = 0.754404621475\n",
      "Test AUC = 0.76771162766    \n",
      "epoch 35: loss = 11.0918483138\n",
      "Train AUC = 0.768912602043\n",
      "Test AUC = 0.771740608448    \n",
      "epoch 36: loss = 11.0291213393\n",
      "Train AUC = 0.774081593102\n",
      "Test AUC = 0.775226258967    \n",
      "epoch 37: loss = 11.0150622129\n",
      "Train AUC = 0.775065990936\n",
      "Test AUC = 0.775438872962    \n",
      "epoch 38: loss = 10.9892725348\n",
      "Train AUC = 0.776733992117\n",
      "Test AUC = 0.776090469243    \n",
      "epoch 39: loss = 11.0244336128\n",
      "Train AUC = 0.774423741368\n",
      "Test AUC = 0.779291561906    \n",
      "epoch 40: loss = 10.9858537912\n",
      "Train AUC = 0.776794273075\n",
      "Test AUC = 0.772046820767    \n",
      "epoch 41: loss = 11.0794594288\n",
      "Train AUC = 0.771776066352\n",
      "Test AUC = 0.768267133074    \n",
      "epoch 42: loss = 11.0249685645\n",
      "Train AUC = 0.774933248464\n",
      "Test AUC = 0.772905891445    \n",
      "epoch 43: loss = 10.9987171292\n",
      "Train AUC = 0.777085396666\n",
      "Test AUC = 0.775263660793    \n",
      "epoch 44: loss = 11.0136345625\n",
      "Train AUC = 0.775697087799\n",
      "Test AUC = 0.77848188552    \n",
      "epoch 45: loss = 11.0008218884\n",
      "Train AUC = 0.776604326491\n",
      "Test AUC = 0.766837580064    \n",
      "epoch 46: loss = 11.0559854507\n",
      "Train AUC = 0.774180280335\n",
      "Test AUC = 0.769327972858    \n",
      "epoch 47: loss = 11.0192403793\n",
      "Train AUC = 0.776587867527\n",
      "Test AUC = 0.78161514596    \n",
      "epoch 48: loss = 10.9533547759\n",
      "Train AUC = 0.779548851202\n",
      "Test AUC = 0.777926892825    \n",
      "epoch 49: loss = 10.9951022863\n",
      "Train AUC = 0.776757474606\n",
      "Test AUC = 0.774674932879    \n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    # Initialize\n",
    "    tf.initialize_all_variables().run()\n",
    "    mean_loss = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        pred_all = []\n",
    "        truth_all = []\n",
    "        for batch_no in range(data_generator.get_train_batch_num()):\n",
    "            batch_inputs, batch_labels = data_generator.get_train_batch()\n",
    "            reset_state.run()    # new sequence\n",
    "            for input_window, label_window in zip(batch_inputs, batch_labels):\n",
    "                \n",
    "                feed_dict = dict()\n",
    "                for i in range(time_window):\n",
    "                    feed_dict[inputs[i]] = input_window[i]\n",
    "                    feed_dict[question_labels[i]] = label_window[i][:, 0:num_skills]\n",
    "                    feed_dict[action_labels[i]] = label_window[i][:, num_skills]\n",
    "                \n",
    "                _, l, pred = session.run([optimizer, loss, prediction], feed_dict=feed_dict)\n",
    "                mean_loss += l\n",
    "                label_all = np.concatenate(label_window, axis=0)\n",
    "                # Exclude padded actions\n",
    "                for i in range(len(pred)):\n",
    "                    if np.sum(label_all[i]) != 0:\n",
    "                        pred_all.append(pred[i])\n",
    "                        truth_all.append(label_all[i][num_skills])\n",
    "        \n",
    "        assert len(pred_all) == data_generator._tot_train_record\n",
    "        assert len(truth_all) == data_generator._tot_train_record\n",
    "        print \"epoch \" + str(epoch) + \": loss = \" + str(mean_loss)\n",
    "        print \"Train AUC = \" + str(metrics.roc_auc_score(truth_all, pred_all))\n",
    "        mean_loss = 0\n",
    "        \n",
    "        if epoch % test_frequency == 0:\n",
    "            pred_all = []\n",
    "            truth_all = []\n",
    "            for batch_no in range(data_generator.get_test_batch_num()):\n",
    "                batch_inputs, batch_labels = data_generator.get_test_batch()\n",
    "                reset_state.run()\n",
    "                for input_window, label_window in zip(batch_inputs, batch_labels):\n",
    "                    feed_dict = dict()\n",
    "                    for i in range(time_window):\n",
    "                        feed_dict[inputs[i]] = input_window[i]\n",
    "                        feed_dict[question_labels[i]] = label_window[i][:, 0:num_skills]\n",
    "                        feed_dict[action_labels[i]] = np.zeros([batch_size, ])      # No need to give the target\n",
    "                    \n",
    "                    pred = test_prediction.eval(feed_dict)\n",
    "                    label_all = np.concatenate(label_window, axis=0)\n",
    "                    # Exclude padded actions\n",
    "                    for i in range(len(pred)):\n",
    "                        if np.sum(label_all[i]) != 0:\n",
    "                            pred_all.append(pred[i])\n",
    "                            truth_all.append(label_all[i][num_skills])\n",
    "            assert len(pred_all) == data_generator._tot_test_record\n",
    "            assert len(truth_all) == data_generator._tot_test_record\n",
    "            print \"Test AUC = \" + str(metrics.roc_auc_score(truth_all, pred_all)) + \"    \"\n",
    "            \n",
    "            if epoch % output_frequency == 0:\n",
    "                pred_action = file(\"prediction@epoch_\" + str(epoch) + '.csv', 'w')\n",
    "                pred_action.write('pred,truth\\n')\n",
    "                for i in range(len(pred_all)):\n",
    "                    pred_action.write(str(pred_all[i]) + ',' + str(truth_all[i]) + '\\n')\n",
    "                pred_action.flush()\n",
    "                pred_action.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Log\n",
    "[Set 1]  \n",
    "epoch 0: loss = 567.766662002    Test AUC = 0.769160009625    \n",
    "epoch 1: loss = 561.678450704    Test AUC = 0.792352762107    \n",
    "epoch 2: loss = 559.633854687    Test AUC = 0.802843348663    \n",
    "epoch 3: loss = 558.392439961    Test AUC = 0.809475230732    \n",
    "epoch 4: loss = 557.570638657    Test AUC = 0.813102115054    \n",
    "epoch 5: loss = 556.914604604    Test AUC = 0.814558511321    \n",
    "epoch 6: loss = 556.38486594     Test AUC = 0.814910906558    \n",
    "epoch 7: loss = 555.848721504    Test AUC = 0.815193798888    \n",
    "epoch 8: loss = 555.357498288    Test AUC = 0.815226028808    \n",
    "epoch 9: loss = 554.890223265    Test AUC = 0.814296198834    \n",
    "epoch 10: loss = 554.397933245   Test AUC = 0.81007721421    \n",
    "\n",
    "[Set 2]  \n",
    "epoch 0: loss = 332.910510778\n",
    "Test AUC = 0.674394648502    \n",
    "epoch 1: loss = 330.410343766\n",
    "Test AUC = 0.769532576628    \n",
    "epoch 2: loss = 328.604476213\n",
    "Test AUC = 0.789874732514    \n",
    "epoch 3: loss = 327.884168208\n",
    "Test AUC = 0.799412435034    \n",
    "epoch 4: loss = 327.37021488\n",
    "Test AUC = 0.80563279165    \n",
    "epoch 5: loss = 326.957753122\n",
    "Test AUC = 0.809847454097    \n",
    "epoch 6: loss = 326.599114776\n",
    "Test AUC = 0.814033516832    \n",
    "epoch 7: loss = 326.286964297\n",
    "Test AUC = 0.816342685584    \n",
    "epoch 8: loss = 326.063906908\n",
    "Test AUC = 0.816886560374    \n",
    "epoch 9: loss = 325.838874578\n",
    "Test AUC = 0.817185123433    \n",
    "epoch 10: loss = 325.609891176\n",
    "Test AUC = 0.817061722639    \n",
    "epoch 11: loss = 325.378716528\n",
    "Test AUC = 0.816330687684    \n",
    "epoch 12: loss = 325.160521328\n",
    "Test AUC = 0.815502604237  \n",
    "\n",
    "[Set 3]   \n",
    "epoch 0: loss = 332.835750043\n",
    "Test AUC = 0.718647725378    \n",
    "epoch 1: loss = 330.070697308\n",
    "Test AUC = 0.777552152737    \n",
    "epoch 2: loss = 328.405154288\n",
    "Test AUC = 0.790903209804    \n",
    "epoch 3: loss = 327.849017143\n",
    "Test AUC = 0.800270027838    \n",
    "epoch 4: loss = 327.271425068\n",
    "Test AUC = 0.807289911952    \n",
    "epoch 5: loss = 326.908729076\n",
    "Test AUC = 0.811421508002    \n",
    "epoch 6: loss = 326.619960666\n",
    "Test AUC = 0.813599784297    \n",
    "epoch 7: loss = 326.336533487\n",
    "Test AUC = 0.815815462967    \n",
    "epoch 8: loss = 326.063607395\n",
    "Test AUC = 0.817161858418    \n",
    "epoch 9: loss = 325.825604141\n",
    "Test AUC = 0.81731912728    \n",
    "epoch 10: loss = 325.611338079\n",
    "Test AUC = 0.816776815415    \n",
    "epoch 11: loss = 325.375428975\n",
    "Test AUC = 0.815700919001    \n",
    "epoch 12: loss = 325.164224088\n",
    "Test AUC = 0.814964095224    \n",
    "epoch 13: loss = 324.97730583\n",
    "Test AUC = 0.81452376418   \n",
    "\n",
    "[Set 4] *Current Best Performance!*       \n",
    "epoch 0: loss = 332.829924166\n",
    "Test AUC = 0.664068842513    \n",
    "epoch 1: loss = 330.335905254\n",
    "Test AUC = 0.773178666242    \n",
    "epoch 2: loss = 328.584171534\n",
    "Test AUC = 0.789058629209    \n",
    "epoch 3: loss = 327.841807842\n",
    "Test AUC = 0.800505674067    \n",
    "epoch 4: loss = 327.341259539\n",
    "Test AUC = 0.806546229115    \n",
    "epoch 5: loss = 326.919404149\n",
    "Test AUC = 0.810932691401    \n",
    "epoch 6: loss = 326.596299648\n",
    "Test AUC = 0.813966169368    \n",
    "epoch 7: loss = 326.293343902\n",
    "Test AUC = 0.816576452397    \n",
    "epoch 8: loss = 326.05825007\n",
    "Test AUC = 0.817669991178    \n",
    "epoch 9: loss = 325.900897801\n",
    "Test AUC = 0.817252342778    \n",
    "epoch 10: loss = 325.680461705\n",
    "Test AUC = 0.817112690406    \n",
    "epoch 11: loss = 325.424844742\n",
    "Test AUC = 0.816096301841 \n",
    "\n",
    "[Set 5]\n",
    "Not so good, didn't copy ...\n",
    "\n",
    "[Set 6]\n",
    "epoch 0: loss = 169.199989498\n",
    "Test AUC = 0.694248942971    \n",
    "epoch 1: loss = 168.180951178\n",
    "Test AUC = 0.758151402947    \n",
    "epoch 2: loss = 167.458352327\n",
    "Test AUC = 0.776980935197    \n",
    "epoch 3: loss = 167.064126432\n",
    "Test AUC = 0.78581568835    \n",
    "epoch 4: loss = 166.804977894\n",
    "Test AUC = 0.792739664357    \n",
    "epoch 5: loss = 166.616363168\n",
    "Test AUC = 0.79761515851    \n",
    "epoch 6: loss = 166.471256316\n",
    "Test AUC = 0.802961889688    \n",
    "epoch 7: loss = 166.332275212\n",
    "Test AUC = 0.806399543644    \n",
    "epoch 8: loss = 166.206695676\n",
    "Test AUC = 0.80953400805    \n",
    "epoch 9: loss = 166.08298558\n",
    "Test AUC = 0.811898844887    \n",
    "epoch 10: loss = 166.017994046\n",
    "Test AUC = 0.811594235753    \n",
    "epoch 11: loss = 165.930454969\n",
    "Test AUC = 0.814531147132    \n",
    "epoch 12: loss = 165.844239295\n",
    "Test AUC = 0.81505496762    \n",
    "epoch 13: loss = 165.779064178\n",
    "Test AUC = 0.815770047319    \n",
    "epoch 14: loss = 165.706398845\n",
    "Test AUC = 0.816363040141    \n",
    "epoch 15: loss = 165.610882163\n",
    "Test AUC = 0.816819624032    \n",
    "epoch 16: loss = 165.540887356\n",
    "Test AUC = 0.816358639571    \n",
    "epoch 17: loss = 165.505313337\n",
    "Test AUC = 0.816286493099    \n",
    "epoch 18: loss = 165.440673649\n",
    "Test AUC = 0.816875850513    \n",
    "epoch 19: loss = 165.342892051\n",
    "Test AUC = 0.816913585871    \n",
    "epoch 20: loss = 165.273579121\n",
    "Test AUC = 0.815895911042    \n",
    "epoch 21: loss = 165.200304389\n",
    "Test AUC = 0.815416721379    \n",
    "epoch 22: loss = 165.138483346\n",
    "Test AUC = 0.814756179908 \n",
    "\n",
    "[Set 7] dropout    \n",
    "epoch 0: loss = 332.870533764\n",
    "Test AUC = 0.691292566637    \n",
    "epoch 1: loss = 330.289458454\n",
    "Test AUC = 0.776335239547    \n",
    "epoch 2: loss = 328.70829308\n",
    "Test AUC = 0.789275108075    \n",
    "epoch 3: loss = 327.995635629\n",
    "Test AUC = 0.799685036618    \n",
    "epoch 4: loss = 327.551552176\n",
    "Test AUC = 0.805448125492    \n",
    "epoch 5: loss = 327.15153569\n",
    "Test AUC = 0.809385595509    \n",
    "epoch 6: loss = 326.881620884\n",
    "Test AUC = 0.812742414623    \n",
    "epoch 7: loss = 326.648236334\n",
    "Test AUC = 0.814646328776    \n",
    "epoch 8: loss = 326.412108302\n",
    "Test AUC = 0.81654314133    \n",
    "epoch 9: loss = 326.326880515\n",
    "Test AUC = 0.816486054169    \n",
    "epoch 10: loss = 326.117108941\n",
    "Test AUC = 0.816923694333    \n",
    "epoch 11: loss = 325.971155286\n",
    "Test AUC = 0.816806584182    \n",
    "epoch 12: loss = 325.817107141\n",
    "Test AUC = 0.816793382851    \n",
    "epoch 13: loss = 325.739503741\n",
    "Test AUC = 0.817429746999    \n",
    "epoch 14: loss = 325.577210069\n",
    "Test AUC = 0.816653405995    \n",
    "epoch 15: loss = 325.47225219\n",
    "Test AUC = 0.815456828841    \n",
    "epoch 16: loss = 325.354509473\n",
    "Test AUC = 0.814910316196  \n",
    "\n",
    "[Set 8]   More Training Data      \n",
    "epoch 0: loss = 442.598678648\n",
    "Test AUC = 0.714518576918    \n",
    "epoch 1: loss = 438.640891492\n",
    "Test AUC = 0.784920791465    \n",
    "epoch 2: loss = 436.994219601\n",
    "Test AUC = 0.796291819461    \n",
    "epoch 3: loss = 436.168569326\n",
    "Test AUC = 0.80236383621    \n",
    "epoch 4: loss = 435.593809307\n",
    "Test AUC = 0.811327798837    \n",
    "epoch 5: loss = 435.098297596\n",
    "Test AUC = 0.815025850644    \n",
    "epoch 6: loss = 434.73207444\n",
    "Test AUC = 0.818647892153    \n",
    "epoch 7: loss = 434.444538713\n",
    "Test AUC = 0.819475078187    \n",
    "epoch 8: loss = 434.154552817\n",
    "Test AUC = 0.823203870896    \n",
    "epoch 9: loss = 434.021283925\n",
    "Test AUC = 0.823570042129    \n",
    "epoch 10: loss = 433.785658121\n",
    "Test AUC = 0.824657541134    \n",
    "epoch 11: loss = 433.603366315\n",
    "Test AUC = 0.823718093871    \n",
    "epoch 12: loss = 433.42709285\n",
    "Test AUC = 0.824673245364    \n",
    "epoch 13: loss = 433.285766006\n",
    "Test AUC = 0.824757491193    \n",
    "epoch 14: loss = 433.117334008\n",
    "Test AUC = 0.824185458514    \n",
    "epoch 15: loss = 432.980303049\n",
    "Test AUC = 0.825087745808    \n",
    "epoch 16: loss = 432.827882051\n",
    "Test AUC = 0.823600250167    \n",
    "epoch 17: loss = 432.721944571\n",
    "Test AUC = 0.822377446865    \n",
    "epoch 18: loss = 432.531647742\n",
    "Test AUC = 0.822054276727    \n",
    "epoch 19: loss = 432.439513385\n",
    "Test AUC = 0.822682079411    \n",
    "epoch 20: loss = 432.288996994\n",
    "Test AUC = 0.821480176982    \n",
    "epoch 21: loss = 432.167292416\n",
    "Test AUC = 0.821338178869    \n",
    "epoch 22: loss = 431.995352268\n",
    "Test AUC = 0.820607282061    \n",
    "epoch 23: loss = 431.919500768\n",
    "Test AUC = 0.819470860341    \n",
    "\n",
    "[Set 9]   \n",
    "epoch 0: loss = 442.436189771\n",
    "Test AUC = 0.753455622017    \n",
    "epoch 1: loss = 438.8392694\n",
    "Test AUC = 0.777074121609    \n",
    "epoch 2: loss = 437.405785859\n",
    "Test AUC = 0.78850172991    \n",
    "epoch 3: loss = 436.760501921\n",
    "Test AUC = 0.795736220818    \n",
    "epoch 4: loss = 436.279414892\n",
    "Test AUC = 0.800484407744    \n",
    "epoch 5: loss = 435.988939106\n",
    "Test AUC = 0.801635255998    \n",
    "epoch 6: loss = 435.712391376\n",
    "Test AUC = 0.806506550885    \n",
    "epoch 7: loss = 435.445997953\n",
    "Test AUC = 0.809259519829    \n",
    "epoch 8: loss = 435.241165936\n",
    "Test AUC = 0.810997590907    \n",
    "epoch 9: loss = 435.003818512\n",
    "Test AUC = 0.813937628272    \n",
    "epoch 10: loss = 434.881675243\n",
    "Test AUC = 0.814369496473    \n",
    "epoch 11: loss = 434.726149261\n",
    "Test AUC = 0.81523300481    \n",
    "epoch 12: loss = 434.620957196\n",
    "Test AUC = 0.815983006419    \n",
    "epoch 13: loss = 434.479012251\n",
    "Test AUC = 0.816051364493    \n",
    "epoch 14: loss = 434.409355283\n",
    "Test AUC = 0.816258395236    \n",
    "epoch 15: loss = 434.268797874\n",
    "Test AUC = 0.818516805702    \n",
    "epoch 16: loss = 434.227459311\n",
    "Test AUC = 0.817730055713    \n",
    "epoch 17: loss = 434.108650446\n",
    "Test AUC = 0.817608971351    \n",
    "epoch 18: loss = 434.019179344\n",
    "Test AUC = 0.816177514025    \n",
    "epoch 19: loss = 433.954689682\n",
    "Test AUC = 0.817096863738    \n",
    "epoch 20: loss = 433.933077633\n",
    "Test AUC = 0.817612623597    \n",
    "epoch 21: loss = 433.827710927\n",
    "Test AUC = 0.818286525829    \n",
    "epoch 22: loss = 433.756803811\n",
    "Test AUC = 0.817414341931  \n",
    "\n",
    "[Set 10]    \n",
    "epoch 0: loss = 442.226243258\n",
    "Train AUC = 0.639660286102\n",
    "Test AUC = 0.759812569788    \n",
    "epoch 1: loss = 438.143710256\n",
    "Train AUC = 0.760106085819\n",
    "Test AUC = 0.786862651556    \n",
    "epoch 2: loss = 436.413987935\n",
    "Train AUC = 0.787821990708\n",
    "Test AUC = 0.802491135186    \n",
    "epoch 3: loss = 435.555881798\n",
    "Train AUC = 0.800702039632\n",
    "Test AUC = 0.809645335167    \n",
    "epoch 4: loss = 434.980609298\n",
    "Train AUC = 0.808693899847\n",
    "Test AUC = 0.816356077532    \n",
    "epoch 5: loss = 434.456144571\n",
    "Train AUC = 0.816129886448\n",
    "Test AUC = 0.819674311283    \n",
    "epoch 6: loss = 434.063976288\n",
    "Train AUC = 0.821731286341\n",
    "Test AUC = 0.82338996213    \n",
    "epoch 7: loss = 433.697606802\n",
    "Train AUC = 0.82697030444\n",
    "Test AUC = 0.82482351228    \n",
    "epoch 8: loss = 433.43130964\n",
    "Train AUC = 0.830989103501\n",
    "Test AUC = 0.824790458538    \n",
    "epoch 9: loss = 433.12830174\n",
    "Train AUC = 0.835590257196\n",
    "Test AUC = 0.824643925436    \n",
    "epoch 10: loss = 432.815392196\n",
    "Train AUC = 0.840240887547\n",
    "Test AUC = 0.823530914905 \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

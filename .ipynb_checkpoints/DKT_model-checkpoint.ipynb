{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DKT Model\n",
    "\n",
    "This file trains a DKT model with Assistment data and tests the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data we pre-processed. It contains each student's action sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'student', u'skill', u'correct'], dtype='object')\n",
      "338001 problem records\n",
      "123 skills\n",
      "4163 students\n",
      "220802 correct answers\n"
     ]
    }
   ],
   "source": [
    "# Limit the data size\n",
    "skill_cut = 150        # limit skill amounts\n",
    "student_cut = 5000    # limit sequences\n",
    "\n",
    "dataset = pd.read_csv(\"Assistments/assistment_for_dkt.csv\")\n",
    "dataset = dataset[dataset['skill'] < skill_cut]\n",
    "print dataset.columns\n",
    "num_records = len(dataset)\n",
    "num_skills = len(dataset['skill'].value_counts())\n",
    "num_actions = 2 * num_skills    # action: every skill correct/wrong\n",
    "num_labels = num_skills + 1     # one-hot question, plus one bit for correct/wrong\n",
    "num_students = len(dataset['student'].value_counts())\n",
    "print str(num_records) + \" problem records\"\n",
    "print str(num_skills) + \" skills\"\n",
    "print str(num_students) + \" students\"\n",
    "print str(np.sum(dataset['correct'].values)) + \" correct answers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following LSTM is based on the one in the Udacity Assignment. The structure of LSTM is the one introduced in this [article](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper parameters to Tune\n",
    "num_hidden = 200\n",
    "init_mean = 0\n",
    "init_stddev = 0.001\n",
    "# batch_size sequences, with the length of time_window\n",
    "batch_size = 100\n",
    "time_window = 100\n",
    "# Training\n",
    "# We are using Adams Optimizer, so no hyperparameter.\n",
    "clipping_norm = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| set | num hidden | init mean | init stddev | batch size | time window | clipping norm | AUC    | Overfit After |\n",
    "|:---:|:----------:|:---------:|:-----------:|:----------:|-------------|---------------|--------|---------------|\n",
    "|  1  |     200    |     0     |    0.001    |     50     |      50     |       10      | 0.8152 | epoch 8       |\n",
    "|  2  |     200    |     0     |    0.001    |    100     |      50     |       10      | 0.8172 | epoch 9       |\n",
    "|  3  |     200    |     0     |    0.001    |    100     |      50     |        5      | 0.8173 | epoch 9       |\n",
    "|  4  |     200    |     0     |    0.001    |    100     |      50     |        2      | 0.8175 | epoch 8       | \n",
    "|  5  |     200    |   0.01    |    0.001    |    100     |      50     |        2      | 0.8152 | epoch 10      |\n",
    "|  6  |     200    |     0     |    0.001    |    100     |     100     |        2      | 0.8169 | epoch 19      |\n",
    "\n",
    "As for now, none of the hyperparameters seem to have a major influence on the performance. So probaly we'll just leave it here. Note that we are just using the default AdamOptimizer and haven't tuned even one bit.\n",
    "\n",
    "AUC drop in one epoch does not necessarily mean that the model has overfitted. However, our model seems to overfit merely after 10 epoches, therefore we need to add regularization tricks, like dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters: _x for new input, _m for old output, _b for bias\n",
    "    # Input gate\n",
    "    input_x = tf.Variable(tf.truncated_normal([num_actions, num_hidden], init_mean, init_stddev))\n",
    "    input_m = tf.Variable(tf.truncated_normal([num_hidden, num_hidden], init_mean, init_stddev))\n",
    "    input_b = tf.Variable(tf.zeros([1, num_hidden]))\n",
    "    # Forget gate\n",
    "    forget_x = tf.Variable(tf.truncated_normal([num_actions, num_hidden], init_mean, init_stddev))\n",
    "    forget_m = tf.Variable(tf.truncated_normal([num_hidden, num_hidden], init_mean, init_stddev))\n",
    "    forget_b = tf.Variable(tf.zeros([1, num_hidden]))\n",
    "    # Update cell:                             \n",
    "    update_x = tf.Variable(tf.truncated_normal([num_actions, num_hidden], init_mean, init_stddev))\n",
    "    update_m = tf.Variable(tf.truncated_normal([num_hidden, num_hidden], init_mean, init_stddev))\n",
    "    update_b = tf.Variable(tf.zeros([1, num_hidden]))\n",
    "    # Output gate:\n",
    "    output_x = tf.Variable(tf.truncated_normal([num_actions, num_hidden], init_mean, init_stddev))\n",
    "    output_m = tf.Variable(tf.truncated_normal([num_hidden, num_hidden], init_mean, init_stddev))\n",
    "    output_b = tf.Variable(tf.zeros([1, num_hidden]))\n",
    "    # Variables saving state across the sequence (length: time_window).\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_hidden]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_hidden]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    classify_w = tf.Variable(tf.truncated_normal([num_hidden, num_skills], init_mean, init_stddev))\n",
    "    classify_b = tf.Variable(tf.zeros([num_skills]))\n",
    "  \n",
    "    def lstm_cell(i, o, state):\n",
    "        # input, last/saved_output, last/saved_state\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, input_x) + tf.matmul(o, input_m) + input_b)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, forget_x) + tf.matmul(o, forget_m) + forget_b)\n",
    "        update = tf.tanh(tf.matmul(i, update_x) + tf.matmul(o, update_m) + update_b)\n",
    "        state = forget_gate * state + input_gate * update\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, output_x) + tf.matmul(o, output_m) + output_b)\n",
    "        # return new_output, new_state\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    inputs = list()\n",
    "    question_labels = list()    # only when training\n",
    "    action_labels = list()\n",
    "    for _ in range(time_window):\n",
    "        inputs.append(tf.placeholder(tf.float32, shape=[batch_size, num_actions]))\n",
    "        question_labels.append(tf.placeholder(tf.float32, shape=[batch_size, num_skills]))\n",
    "        action_labels.append(tf.placeholder(tf.float32, shape=[batch_size, ]))\n",
    "    \n",
    "    # State resets when starting a new sequence\n",
    "    reset_state = tf.group(saved_output.assign(tf.zeros([batch_size, num_hidden])),\n",
    "                           saved_state.assign(tf.zeros([batch_size, num_hidden])))\n",
    "    \n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across different segment of a sequence\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), classify_w, classify_b)\n",
    "        # logits of the actual encountered problem:\n",
    "        logits_of_interest = tf.reduce_sum(tf.mul(logits, tf.concat(0, question_labels)), 1)\n",
    "        truth = tf.reshape(tf.concat(0, action_labels), [-1])    # flatten\n",
    "        # binary cross entropy: padding would introduce some constant loss\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits_of_interest, truth))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    gradients, var = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, clipping_norm)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, var))\n",
    "    \n",
    "    prediction = tf.sigmoid(logits_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genrating input sequences for LSTM is a bit complicated. The general idea is first take a batch of students then pad their sequence to the same length. When feeding to LSTM, we feed one \"window\"(time interval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataGenerator(object):\n",
    "    def __init__(self, dataset, train_ratio):\n",
    "        # convert file to sequence\n",
    "        dataset = dataset.values\n",
    "        seqs = list()\n",
    "        last_student = -1\n",
    "        print dataset.shape\n",
    "        for i in range(len(dataset)):\n",
    "            if dataset[i][0] != last_student:    # a new student\n",
    "                last_student = dataset[i][0]\n",
    "                seqs.append([(dataset[i][1], dataset[i][2])])  # (skill, correct)\n",
    "            else:     # same student\n",
    "                seqs[-1].append((dataset[i][1], dataset[i][2]))\n",
    "        del dataset\n",
    "        \n",
    "        tot_seqs = min(len(seqs), student_cut)\n",
    "        print \"total: %d sequences\" % tot_seqs\n",
    "        \n",
    "        # split train and test\n",
    "        train_size = int(tot_seqs * train_ratio)\n",
    "        train_seq_cnt = 0\n",
    "        self._train_seqs = list()\n",
    "        for i in range(train_size):\n",
    "            self._train_seqs.append(seqs[i])\n",
    "            train_seq_cnt += len(seqs[i])\n",
    "        test_seq_cnt = 0\n",
    "        self._test_seqs = list()\n",
    "        for i in range(train_size, tot_seqs):\n",
    "            self._test_seqs.append(seqs[i])\n",
    "            test_seq_cnt += len(seqs[i])\n",
    "        print \"%d records for train\" % train_seq_cnt\n",
    "        print \"%d records for test\" % test_seq_cnt\n",
    "        \n",
    "        # takes around 2GB memory:\n",
    "        self._train_inputs = []\n",
    "        self._train_labels = []\n",
    "        self.generate_batch(self._train_seqs, self._train_inputs, self._train_labels)\n",
    "        \n",
    "        self._test_inputs = []\n",
    "        self._test_labels = []\n",
    "        self.generate_batch(self._test_seqs, self._test_inputs, self._test_labels)\n",
    "        \n",
    "        print \"all batch generated\"\n",
    "        \n",
    "        self._train_cursor = -1\n",
    "        self._test_cursor = -1\n",
    "        \n",
    "    def get_train_batch_num(self):\n",
    "        return len(self._train_inputs)\n",
    "    \n",
    "    def get_test_batch_num(self):\n",
    "        return len(self._test_inputs)\n",
    "    \n",
    "    def get_train_batch(self):\n",
    "        self._train_cursor += 1\n",
    "        if self._train_cursor == len(self._train_inputs):\n",
    "            self._train_cursor = 0\n",
    "        return self._train_inputs[self._train_cursor], self._train_labels[self._train_cursor]\n",
    "    \n",
    "    def get_test_batch(self):\n",
    "        self._test_cursor += 1\n",
    "        if self._test_cursor == len(self._test_inputs):\n",
    "            self._test_cursor = 0\n",
    "        return self._test_inputs[self._test_cursor], self._test_labels[self._test_cursor]\n",
    "    \n",
    "    def generate_batch(self, seqs_pool, inputs, labels):\n",
    "        seq_count = len(seqs_pool)\n",
    "        num_batch = int(math.ceil(float(seq_count) / batch_size))\n",
    "        correct_cnt = 0\n",
    "        for start in range(0, seq_count, batch_size):            \n",
    "            end = min(seq_count, start + batch_size)\n",
    "            maxlen = 0\n",
    "            for i in range(start, end):\n",
    "                if maxlen < len(seqs_pool[i]):\n",
    "                    maxlen = len(seqs_pool[i])\n",
    "            num_window = int(math.ceil(float(maxlen) / time_window))\n",
    "            \n",
    "            # setup empty data (i.e., padded with full 0s)\n",
    "            inputs.append([])\n",
    "            labels.append([])\n",
    "            for _ in range(num_window):\n",
    "                inputs[-1].append([])\n",
    "                labels[-1].append([])\n",
    "                for _ in range(time_window):\n",
    "                    inputs[-1][-1].append(np.zeros([batch_size, num_actions], dtype=np.float32))\n",
    "                    labels[-1][-1].append(np.zeros([batch_size, num_labels], dtype=np.float32))\n",
    "            \n",
    "            # fill in data\n",
    "            for i in range(start, end):\n",
    "                pos_in_batch = i - start    # position in batch\n",
    "                seq = seqs_pool[i]\n",
    "                # from back to front\n",
    "                for back_offset in range(1, len(seq) + 1):\n",
    "                    # find the row of the record\n",
    "                    window_offset = - int(math.ceil(float(back_offset) / time_window))\n",
    "                    frame_offset = - back_offset % time_window\n",
    "                    if frame_offset == 0:\n",
    "                        frame_offset = - time_window\n",
    "                    # code the record by setting ones\n",
    "                    record = seq[- back_offset]\n",
    "                    labels[-1][window_offset][frame_offset][pos_in_batch][record[0]] = 1\n",
    "                    labels[-1][window_offset][frame_offset][pos_in_batch][num_skills] = record[1]\n",
    "                    \n",
    "                    input_back_offset = back_offset - 1    # skew input backward 1 time step\n",
    "                    if input_back_offset == 0:\n",
    "                        continue\n",
    "                    input_window_offset = - int(math.ceil(float(input_back_offset) / time_window))\n",
    "                    input_frame_offset = - input_back_offset % time_window\n",
    "                    if input_frame_offset == 0:\n",
    "                        input_frame_offset = - time_window\n",
    "                    inputs[-1][input_window_offset][input_frame_offset][pos_in_batch][2 * record[0] + record[1]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following session trains and runs the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(338001, 3)\n",
      "total: 4163 sequences\n",
      "206404 records for train\n",
      "131597 records for test\n",
      "all batch generated\n"
     ]
    }
   ],
   "source": [
    "# Running Specifications\n",
    "num_epochs = 50\n",
    "test_frequency = 1\n",
    "train_ratio = 0.6\n",
    "\n",
    "data_generator = DataGenerator(dataset, train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = 169.199989498\n",
      "Test AUC = 0.694248942971    \n",
      "epoch 1: loss = 168.180951178\n",
      "Test AUC = 0.758151402947    \n",
      "epoch 2: loss = 167.458352327\n",
      "Test AUC = 0.776980935197    \n",
      "epoch 3: loss = 167.064126432\n",
      "Test AUC = 0.78581568835    \n",
      "epoch 4: loss = 166.804977894\n",
      "Test AUC = 0.792739664357    \n",
      "epoch 5: loss = 166.616363168\n",
      "Test AUC = 0.79761515851    \n",
      "epoch 6: loss = 166.471256316\n",
      "Test AUC = 0.802961889688    \n",
      "epoch 7: loss = 166.332275212\n",
      "Test AUC = 0.806399543644    \n",
      "epoch 8: loss = 166.206695676\n",
      "Test AUC = 0.80953400805    \n",
      "epoch 9: loss = 166.08298558\n",
      "Test AUC = 0.811898844887    \n",
      "epoch 10: loss = 166.017994046\n",
      "Test AUC = 0.811594235753    \n",
      "epoch 11: loss = 165.930454969\n",
      "Test AUC = 0.814531147132    \n",
      "epoch 12: loss = 165.844239295\n",
      "Test AUC = 0.81505496762    \n",
      "epoch 13: loss = 165.779064178\n",
      "Test AUC = 0.815770047319    \n",
      "epoch 14: loss = 165.706398845\n",
      "Test AUC = 0.816363040141    \n",
      "epoch 15: loss = 165.610882163\n",
      "Test AUC = 0.816819624032    \n",
      "epoch 16: loss = 165.540887356\n",
      "Test AUC = 0.816358639571    \n",
      "epoch 17: loss = 165.505313337\n",
      "Test AUC = 0.816286493099    \n",
      "epoch 18: loss = 165.440673649\n",
      "Test AUC = 0.816875850513    \n",
      "epoch 19: loss = 165.342892051\n",
      "Test AUC = 0.816913585871    \n",
      "epoch 20: loss = 165.273579121\n",
      "Test AUC = 0.815895911042    \n",
      "epoch 21: loss = 165.200304389\n",
      "Test AUC = 0.815416721379    \n",
      "epoch 22: loss = 165.138483346\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    # Initialize\n",
    "    tf.initialize_all_variables().run()\n",
    "    mean_loss = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_no in range(data_generator.get_train_batch_num()):\n",
    "            batch_inputs, batch_labels = data_generator.get_train_batch()\n",
    "            reset_state.run()    # new sequence\n",
    "            for input_window, label_window in zip(batch_inputs, batch_labels):\n",
    "                \n",
    "                feed_dict = dict()\n",
    "                for i in range(time_window):\n",
    "                    feed_dict[inputs[i]] = input_window[i]\n",
    "                    feed_dict[question_labels[i]] = label_window[i][:, 0:num_skills]\n",
    "                    feed_dict[action_labels[i]] = label_window[i][:, num_skills]\n",
    "                \n",
    "                _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "                mean_loss += l\n",
    "        \n",
    "        print \"epoch \" + str(epoch) + \": loss = \" + str(mean_loss)\n",
    "        mean_loss = 0\n",
    "        \n",
    "        if epoch % test_frequency == 0:\n",
    "            pred_all = []\n",
    "            truth_all = []\n",
    "            for batch_no in range(data_generator.get_test_batch_num()):\n",
    "                batch_inputs, batch_labels = data_generator.get_test_batch()\n",
    "                reset_state.run()\n",
    "                for input_window, label_window in zip(batch_inputs, batch_labels):\n",
    "                    feed_dict = dict()\n",
    "                    for i in range(time_window):\n",
    "                        feed_dict[inputs[i]] = input_window[i]\n",
    "                        feed_dict[question_labels[i]] = label_window[i][:, 0:num_skills]\n",
    "                        feed_dict[action_labels[i]] = np.zeros([batch_size, ])      # No need to give the target\n",
    "                    \n",
    "                    pred = prediction.eval(feed_dict)\n",
    "                    label_all = np.concatenate(label_window, axis=0)\n",
    "                    # Exclude padded actions\n",
    "                    for i in range(len(pred)):\n",
    "                        if np.sum(label_all[i]) != 0:\n",
    "                            pred_all.append(pred[i])\n",
    "                            truth_all.append(label_all[i][num_skills])\n",
    "            print \"Test AUC = \" + str(metrics.roc_auc_score(truth_all, pred_all)) + \"    \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Log\n",
    "[Set 1]  \n",
    "epoch 0: loss = 567.766662002    Test AUC = 0.769160009625    \n",
    "epoch 1: loss = 561.678450704    Test AUC = 0.792352762107    \n",
    "epoch 2: loss = 559.633854687    Test AUC = 0.802843348663    \n",
    "epoch 3: loss = 558.392439961    Test AUC = 0.809475230732    \n",
    "epoch 4: loss = 557.570638657    Test AUC = 0.813102115054    \n",
    "epoch 5: loss = 556.914604604    Test AUC = 0.814558511321    \n",
    "epoch 6: loss = 556.38486594     Test AUC = 0.814910906558    \n",
    "epoch 7: loss = 555.848721504    Test AUC = 0.815193798888    \n",
    "epoch 8: loss = 555.357498288    Test AUC = 0.815226028808    \n",
    "epoch 9: loss = 554.890223265    Test AUC = 0.814296198834    \n",
    "epoch 10: loss = 554.397933245   Test AUC = 0.81007721421    \n",
    "\n",
    "[Set 2]  \n",
    "epoch 0: loss = 332.910510778\n",
    "Test AUC = 0.674394648502    \n",
    "epoch 1: loss = 330.410343766\n",
    "Test AUC = 0.769532576628    \n",
    "epoch 2: loss = 328.604476213\n",
    "Test AUC = 0.789874732514    \n",
    "epoch 3: loss = 327.884168208\n",
    "Test AUC = 0.799412435034    \n",
    "epoch 4: loss = 327.37021488\n",
    "Test AUC = 0.80563279165    \n",
    "epoch 5: loss = 326.957753122\n",
    "Test AUC = 0.809847454097    \n",
    "epoch 6: loss = 326.599114776\n",
    "Test AUC = 0.814033516832    \n",
    "epoch 7: loss = 326.286964297\n",
    "Test AUC = 0.816342685584    \n",
    "epoch 8: loss = 326.063906908\n",
    "Test AUC = 0.816886560374    \n",
    "epoch 9: loss = 325.838874578\n",
    "Test AUC = 0.817185123433    \n",
    "epoch 10: loss = 325.609891176\n",
    "Test AUC = 0.817061722639    \n",
    "epoch 11: loss = 325.378716528\n",
    "Test AUC = 0.816330687684    \n",
    "epoch 12: loss = 325.160521328\n",
    "Test AUC = 0.815502604237  \n",
    "\n",
    "[Set 3]   \n",
    "epoch 0: loss = 332.835750043\n",
    "Test AUC = 0.718647725378    \n",
    "epoch 1: loss = 330.070697308\n",
    "Test AUC = 0.777552152737    \n",
    "epoch 2: loss = 328.405154288\n",
    "Test AUC = 0.790903209804    \n",
    "epoch 3: loss = 327.849017143\n",
    "Test AUC = 0.800270027838    \n",
    "epoch 4: loss = 327.271425068\n",
    "Test AUC = 0.807289911952    \n",
    "epoch 5: loss = 326.908729076\n",
    "Test AUC = 0.811421508002    \n",
    "epoch 6: loss = 326.619960666\n",
    "Test AUC = 0.813599784297    \n",
    "epoch 7: loss = 326.336533487\n",
    "Test AUC = 0.815815462967    \n",
    "epoch 8: loss = 326.063607395\n",
    "Test AUC = 0.817161858418    \n",
    "epoch 9: loss = 325.825604141\n",
    "Test AUC = 0.81731912728    \n",
    "epoch 10: loss = 325.611338079\n",
    "Test AUC = 0.816776815415    \n",
    "epoch 11: loss = 325.375428975\n",
    "Test AUC = 0.815700919001    \n",
    "epoch 12: loss = 325.164224088\n",
    "Test AUC = 0.814964095224    \n",
    "epoch 13: loss = 324.97730583\n",
    "Test AUC = 0.81452376418   \n",
    "\n",
    "[Set 4]   \n",
    "Forgot to copy ...\n",
    "\n",
    "[Set 5]\n",
    "Not so good, didn't copy again ...\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
